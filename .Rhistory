dim(x)
x <- fdata.working$data
dim(x)
PenalizedLDA.cv(x=x[,2:3], y=y)
PenalizedLDA.cv(x=x, y=y)
penlda.cv.out <- PenalizedLDA.cv(x=x, y=y)
plot(penlda.cv.out)
lambda.seq <- exp(seq(-2, 2))
lambda.seq <- exp(seq(-2, 2, 0.2))
penlda.cv.out <- PenalizedLDA.cv(x=x, y=y, lambda=lambda.seq)
lambda.seq <- exp(seq(-2, 2, 0.2))
penlda.cv.out <- PenalizedLDA.cv(x=x, y=y, lambda=lambda.seq)
penlda.cv.out <- PenalizedLDA.cv(x=x, y=y, lambdas=lambda.seq)
plot(penlda.cv.out)
lambda.seq <- exp(seq(-4, 2, 0.2))
penlda.cv.out <- PenalizedLDA.cv(x=x, y=y, lambdas=lambda.seq)
plot(penlda.cv.out)
lambda.seq <- exp(seq(-6, 2, 0.2))
penlda.cv.out <- PenalizedLDA.cv(x=x, y=y, lambdas=lambda.seq)
plot(penlda.cv.out)
penlda.cv.out <- PenalizedLDA.cv(x=x, y=y, lambdas=lambda.seq, K = 10)
penlda.cv.out <- PenalizedLDA.cv(x=x, y=y, lambdas=lambda.seq, nfolds = 10)
penlda.cv.out <- PenalizedLDA.cv(x=x, y=y, lambdas=lambda.seq, nfold = 10)
plot(penlda.cv.out)
lambda.seq <- exp(seq(-6, 2, 0.02))
penlda.cv.out <- PenalizedLDA.cv(x=x, y=y, lambdas=lambda.seq, nfold = 10)
plot(penlda.cv.out)
penlda.cv.out$bestlambda
library(MASS)
library(caret)
install.packages("caret")
penlda.cv.out$bestlambda
lambda.seq <- exp(seq(-6, 2, 0.05))
penlda.cv.out <- PenalizedLDA.cv(x=x, y=y, lambdas=lambda.seq, nfold = 10)
plot(penlda.cv.out)
penlda.cv.out$bestlambda
caret::createFolds(class.raw$SLE)
x.test <- x[,fold.id]
y.test <- y[fold.id]
x.train <- x[,-fold.id]
y.train <- y[-fold.id]
set.seed(10)
caret::createFolds(class.raw$SLE)
fold.id <- 1
x.test <- x[,fold.id]
y.test <- y[fold.id]
x.train <- x[,-fold.id]
y.train <- y[-fold.id]
penlda.cv.out <- PenalizedLDA.cv(x=x.train, y=y.train, lambdas=lambda.seq, nfold = 10)
folds.all <- caret::createFolds(class.raw$SLE)
set.seed(10)
folds.all <- caret::createFolds(class.raw$SLE)
fold.id <- 1
x.test <- x[,folds.all[[fold.id]]]
y.test <- y[folds.all[[fold.id]]]
x.train <- x[,-folds.all[[fold.id]]]
y.train <- y[-folds.all[[fold.id]]]
penlda.cv.out <- PenalizedLDA.cv(x=x.train, y=y.train, lambdas=lambda.seq, nfold = 10)
folds.all[[fold.id]]
x.train <- x[,-folds.all[[fold.id]]]
y.train <- y[-folds.all[[fold.id]]]
table(y.train)
### The start of a function
set.seed(10)
folds.all <- caret::createFolds(class.raw$SLE)
lambda.seq <- exp(seq(-6, 2, 0.01))  ### This could be an input, best to suggest evalauating this outide of functions.
fold.id <- 1
folds.all[[fold.id]]
x.test <- x[,folds.all[[fold.id]]]
y.test <- y[folds.all[[fold.id]]]
x.train <- x[,-folds.all[[fold.id]]]
y.train <- y[-folds.all[[fold.id]]]
penlda.cv.out <- PenalizedLDA.cv(x=x.train, y=y.train, lambdas=lambda.seq, nfold = 10)
penlda.cv.out <- PenalizedLDA.cv(x=x, y=y, lambdas=lambda.seq, nfold = 10)
set.seed(10)
folds.all <- caret::createFolds(class.raw$SLE)
lambda.seq <- exp(seq(-6, 2, 0.01))  ### This could be an input, best to suggest evalauating this outide of functions.
fold.id <- 1
folds.all[[fold.id]]
x.test <- x[folds.all[[fold.id]],]
y.test <- y[folds.all[[fold.id]]]
x.train <- x[-folds.all[[fold.id]],]
y.train <- y[-folds.all[[fold.id]]]
penlda.cv.out <- PenalizedLDA.cv(x=x.train, y=y.train, lambdas=lambda.seq, nfold = 10)
lambda.set <- penlda.cv.out$bestlambda
predict(penlda.cv.out, x.test)
predict.penlda(penlda.cv.out, x.test)
fold.id <- 1
folds.all[[fold.id]]
x.test <- x[folds.all[[fold.id]],]
y.test <- y[folds.all[[fold.id]]]
x.train <- x[-folds.all[[fold.id]],]
y.train <- y[-folds.all[[fold.id]]]
predict.penlda(penlda.cv.out, x.test)
predict(penlda.cv.out, x.test)
penlda.out <- PenalizedLDA(x.train, y.train, lambda.set)
penlda.out <- PenalizedLDA(x.train, y.train, lambda=lambda.set)
penlda.out <- PenalizedLDA(x=x.train, y=y.train, lambda=lambda.set)
penlda.cv.out <- PenalizedLDA.cv(x=x.train, y=y.train, lambdas=lambda.seq, nfold = 10)
lambda.set <- penlda.cv.out$bestlambda
penlda.out <- PenalizedLDA(x=x.train, y=y.train, lambda=lambda.set)
penlda.out <- PenalizedLDA(x=x.train, y=y.train)
y.train
penlda.out <- PenalizedLDA(x=x.train, y=y.train, lambda=lambda.set, K=1)
predict(penlda.out, x.test)
penlda.out$discrim
penlda.out$y
penlda.out$
predict(penlda.out, x.test)==y.test
penlda.out$
which(predict(penlda.out, x.test)==y.test)
penlda.out$
penlda.preds <- predict(penlda.out, x.test)
penlda.preds
penlda.preds <- predict(penlda.out, x.test)
penlda.preds
penlda.preds$ypred
penlda.preds$ypred==y.test
mean(penlda.preds$ypred==y.test)
set.seed(10)
folds.all <- caret::createFolds(class.raw$SLE)
lambda.seq <- exp(seq(-6, 2, 0.01))  ### This could be an input, best to suggest evalauating this outide of functions.
acc <- vector()
for(fold.id in 1:folds.n)
{
x.test <- x[folds.all[[fold.id]],]
y.test <- y[folds.all[[fold.id]]]
x.train <- x[-folds.all[[fold.id]],]
y.train <- y[-folds.all[[fold.id]]]
penlda.cv.out <- PenalizedLDA.cv(x=x.train, y=y.train, lambdas=lambda.seq, nfold = 10)
lambda.set <- penlda.cv.out$bestlambda
penlda.out <- PenalizedLDA(x=x.train, y=y.train, lambda=lambda.set, K=1)
penlda.preds <- predict(penlda.out, x.test)
acc[fold.id] <- mean(penlda.preds$ypred==y.test)  ### There does not seem to be a way to get posterior probabilities, so we may not include this
}
folds.all <- caret::createFolds(class.raw$SLE, k = folds.n)
folds.n <- 10
folds.all <- caret::createFolds(class.raw$SLE, k = folds.n)
lambda.seq <- exp(seq(-6, 2, 0.01))  ### This could be an input, best to suggest evalauating this outide of functions.
acc <- vector()
for(fold.id in 1:folds.n)
{
x.test <- x[folds.all[[fold.id]],]
y.test <- y[folds.all[[fold.id]]]
x.train <- x[-folds.all[[fold.id]],]
y.train <- y[-folds.all[[fold.id]]]
penlda.cv.out <- PenalizedLDA.cv(x=x.train, y=y.train, lambdas=lambda.seq, nfold = 10)
lambda.set <- penlda.cv.out$bestlambda
penlda.out <- PenalizedLDA(x=x.train, y=y.train, lambda=lambda.set, K=1)
penlda.preds <- predict(penlda.out, x.test)
acc[fold.id] <- mean(penlda.preds$ypred==y.test)  ### There does not seem to be a way to get posterior probabilities, so we may not include this
}
boxplot(acc)
### Enforcing a fused penalty
set.seed(10)
folds.n <- 10
folds.all <- caret::createFolds(class.raw$SLE, k = folds.n)
lambda.seq <- exp(seq(-6, 2, 0.01))  ### This could be an input, best to suggest evalauating this outide of functions.
acc <- vector()
for(fold.id in 1:folds.n)
{
x.test <- x[folds.all[[fold.id]],]
y.test <- y[folds.all[[fold.id]]]
x.train <- x[-folds.all[[fold.id]],]
y.train <- y[-folds.all[[fold.id]]]
penlda.cv.out <- PenalizedLDA.cv(x=x.train, y=y.train, lambdas=lambda.seq, nfold = 10, type='ordered')
lambda.set <- penlda.cv.out$bestlambda
penlda.out <- PenalizedLDA(x=x.train, y=y.train, lambda=lambda.set, K=1, type='ordered')
penlda.preds <- predict(penlda.out, x.test)
acc[fold.id] <- mean(penlda.preds$ypred==y.test)  ### There does not seem to be a way to get posterior probabilities, so we may not include this
}
boxplot(acc)  ### Got about 72% mean test set accuracy in first run
source('C:/Users/Robert/Dropbox/Northern Arizona University/Github_Repos/Thermogram_Toolkit/Data_Import_Cleaning.R')
install.packages(c("caret", "fda", "fda.usc", "penalizedLDA"))
### Libraries
library(caret)
library(fda)
library(fda.usc)
library(dplyr)
### Data
### Establish a clear set of inputs that can be used generally for statistical estimation.
### We can also start working to establish .Rdata files that have the data frame stored correctly.
data.raw <- read.table('lupustherm.txt')
temperatures <- seq(45, 90, 0.1)
thermogram.length <- length(temperatures)
samples.n <- dim(data.raw)[1]
thermograms.raw <- data.raw %>% select(1:thermogram.length)
class.raw <- data.raw %>% select(-(1:thermogram.length))
### Provide Names for any columns that are classifiers information.  So here, we indicate lupus/non-lupus
colnames(class.raw) <- 'SLE'
class.raw$SLE <- factor(class.raw$SLE)
### Visualization
### We can probably produce some nice visualization using ggplot.
# cols = rainbow(samples.n)
# plot(temperatures, thermograms.raw[1,], type='l', ylim = c(-0.25, 0.5), col=cols[1])
# for(i in 2:samples.n) lines(temperatures, thermograms.raw[i,], col=cols[i])
### Functional Data
### To be used for smoothing and derivatives
fdata.raw <- fdata(thermograms.raw, argvals=temperatures)
plot(fdata.raw)
######## Smoothing ############
### Choose a Basis
# full.spline <- create.bspline.basis(range=range(temperatures), nbasis = thermogram.length+2)
### We can tune from basis = 2 to length(argvals))
basis <- 125 ### Tuneable, if basis=length(arvals) then use raw.
reduced.basis.spline <- create.bspline.basis(range=range(temperatures), nbasis = basis+2)
Smoothing.Matrix <- S.basis(temperatures, basis=reduced.basis.spline)
fdata.smooth <- fdata.raw
fdata.smooth$data <- fdata.raw$data%*%Smoothing.Matrix
plot(fdata.smooth)
cbind(fdata.smooth$data[,1], fdata.raw$data[,1])
fdata.working <- fdata.smooth ### Working functional data set
toolkit.ws <- data.frame(Class = class.raw$SLE, X = fdata.working$data)  ### data.frame ws
colnames(toolkit.ws) <- c('Class', temperatures)
################################
### Data flow/cleaning is all above.
### Below is me starting to work on functions that should call the working data set.
################################
######### Function #############
## I will start by doing something outside the scope of 578.  I will use Penalized-LDA
library('penalizedLDA')
fdata.working <- fdata.raw ### Working functional data set
x <- fdata.working$data
dim(x)
y <- as.numeric(class.raw$SLE)
penlda.cv.out <- PenalizedLDA.cv(x=x, y=y)
lambda.seq <- exp(seq(-6, 2, 0.01))  ### This could be an input, best to suggest evalauating this outide of functions.
penlda.cv.out <- PenalizedLDA.cv(x=x, y=y, lambdas=lambda.seq, nfold = 10)
plot(penlda.cv.out)
penlda.cv.out$bestlambda
set.seed(10)
folds.all <- caret::createFolds(class.raw$SLE)
lambda.seq <- exp(seq(-6, 2, 0.01))  ### This could be an input, best to suggest evalauating this outide of functions.
fold.id <- 1
folds.all[[fold.id]]
x.test <- x[folds.all[[fold.id]],]
y.test <- y[folds.all[[fold.id]]]
x.train <- x[-folds.all[[fold.id]],]
y.train <- y[-folds.all[[fold.id]]]
penlda.cv.out <- PenalizedLDA.cv(x=x.train, y=y.train, lambdas=lambda.seq, nfold = 10)
lambda.set <- penlda.cv.out$bestlambda
penlda.out <- PenalizedLDA(x=x.train, y=y.train, lambda=lambda.set, K=1)
penlda.preds <- predict(penlda.out, x.test)
acc <- mean(penlda.preds$ypred==y.test)  ### There does not seem to be a way to get posterior probabilities, so we may not include this
penlda.out$discrim
penlda.out$discrim %*% x.test[1,]
penlda.out$discrim %*% x.test[,1]
x.test[1,]%*%penlda.out$discrim
penlda.preds <- predict(penlda.out, x.test)
x.test[2,]%*%penlda.out$discrim
x.test%*%penlda.out$discrim
exp(x.test%*%penlda.out$discrim)
1/(1+exp(x.test%*%penlda.out$discrim))
1/(1+exp(x.test%*%penlda.out$discrim)) > 0.5
1/(1+exp(x.test%*%penlda.out$discrim))
1/(1+exp(-x.test%*%penlda.out$discrim))
plot(penlda.out$discrim)
1/(1+exp(-t(x.test)%*%penlda.out$discrim))
1/(1+exp(-x.test%*%penlda.out$discrim))
post <- 1/(1+exp(-x.test%*%penlda.out$discrim))
cbind(post, penlda.preds)
cbind(post, penlda.preds$ypred)
x.test%*%penlda.out$discrim
t(x.test)%*%penlda.out$discrim
penlda.out$discrim%*%t(x.test)
penlda.out$discrim%*%x.test
penlda.out$discrim%*%x.test[,1]
penlda.out$discrim%*%x.test[1,]
x.test%*%penlda.out$discrim
1/(1+x.test%*%penlda.out$discrim)
1/(1+exp(x.test%*%penlda.out$discrim))
1/(1+exp(-x.test%*%penlda.out$discrim))
probs <- 1/(1+exp(-x.test%*%penlda.out$discrim))
cbind(probs>0, penlda.preds$ypred)
cbind(probs>0.5, penlda.preds$ypred)
probs>0.5
predict <- if(probs>0.5) 1
predict <- probs>0.5
predict.test <- probs>0.5
factor(predict.test)
factor(predict.test, c(0,1))
factor(predict.test, c(TRUE, FALSE), c(0,1))
y.test
y.test-1
table(y.test-1, predict.test)
mean(y.test-1==predict.test)
fold.id <- 2
folds.all[[fold.id]]
x.test <- x[folds.all[[fold.id]],]
y.test <- y[folds.all[[fold.id]]]
x.train <- x[-folds.all[[fold.id]],]
y.train <- y[-folds.all[[fold.id]]]
penlda.cv.out <- PenalizedLDA.cv(x=x.train, y=y.train, lambdas=lambda.seq, nfold = 10)
lambda.set <- penlda.cv.out$bestlambda
penlda.out <- PenalizedLDA(x=x.train, y=y.train, lambda=lambda.set, K=1)
plot(penlda.out$discrim)
probs <- 1/(1+exp(-x.test%*%penlda.out$discrim))
predict.test <- probs>0.5
factor(predict.test, c(TRUE, FALSE), c(0,1))
y.test-1
table(y.test-1, predict.test)
mean(y.test-1==predict.test)
penlda.preds <- predict(penlda.out, x.test)
cbind(post, penlda.preds$ypred)
mean(penlda.preds$ypred==y.test)
cbind(probs, penlda.preds$ypred)
probs <- x.test%*%penlda.out$discrim
probs
penlda.preds <- predict(penlda.out, x.test)
predict.penlda
probs <- (x.test-mean(x.test))%*%penlda.out$discrim
probs
probs
predict.test <- probs>0.5
factor(predict.test, c(TRUE, FALSE), c(0,1))
y.test-1
table(y.test-1, predict.test)
mean(y.test-1==predict.test)
cbind(probs, penlda.preds$ypred)
penlda.preds <- predict(penlda.out, x.test)
acc <- mean(penlda.preds$ypred==y.test)  ### There does not seem to be a way to get posterior probabilities, so we may not include this
predict.penlda
mean(y.test-1==predict.test)
probs <- (x.test-mean(x.train))%*%penlda.out$discrim
probs
predict.test <- probs>0.5
factor(predict.test, c(TRUE, FALSE), c(0,1))
y.test-1
table(y.test-1, predict.test)
mean(y.test-1==predict.test)
plot(penlda.out$discrim)
probs <- (x.test-mean(x.train))%*%penlda.out$discrim
probs
predict.test <- probs>0
factor(predict.test, c(TRUE, FALSE), c(0,1))
y.test-1
table(y.test-1, predict.test)
mean(y.test-1==predict.test)
mean(y.test-1==predict.test)
acc <- mean(penlda.preds$ypred==y.test)  ### There does not seem to be a way to get posterior probabilities, so we may not include this
predict.penlda
scale(x.test, mean(x.train), sd(x.train))
apply(x.train, 2, mean)
penlda.out$wcsd.x
xte <- scale(xtest, center = apply(x.train, 2, mean), scale = penlda.out$wcsd.x)
xte <- scale(x.test, center = apply(x.train, 2, mean), scale = penlda.out$wcsd.x)
probs <- (xte%*%penlda.out$discrim
probs <- xte%*%penlda.out$discrim
probs <- xte%*%penlda.out$discrim
probs
predict.test <- probs>0
factor(predict.test, c(TRUE, FALSE), c(0,1))
y.test-1
table(y.test-1, predict.test)
mean(y.test-1==predict.test)
### Libraries
library(caret)
library(fda)
library(fda.usc)
library(dplyr)
### Data
### Establish a clear set of inputs that can be used generally for statistical estimation.
### We can also start working to establish .Rdata files that have the data frame stored correctly.
data.raw <- read.table('lupustherm.txt')
temperatures <- seq(45, 90, 0.1)
thermogram.length <- length(temperatures)
samples.n <- dim(data.raw)[1]
thermograms.raw <- data.raw %>% select(1:thermogram.length)
class.raw <- data.raw %>% select(-(1:thermogram.length))
### Provide Names for any columns that are classifiers information.  So here, we indicate lupus/non-lupus
colnames(class.raw) <- 'SLE'
class.raw$SLE <- factor(class.raw$SLE)
### Visualization
### We can probably produce some nice visualization using ggplot.
# cols = rainbow(samples.n)
# plot(temperatures, thermograms.raw[1,], type='l', ylim = c(-0.25, 0.5), col=cols[1])
# for(i in 2:samples.n) lines(temperatures, thermograms.raw[i,], col=cols[i])
### Functional Data
### To be used for smoothing and derivatives
fdata.raw <- fdata(thermograms.raw, argvals=temperatures)
plot(fdata.raw)
######## Smoothing ############
### Choose a Basis
# full.spline <- create.bspline.basis(range=range(temperatures), nbasis = thermogram.length+2)
### We can tune from basis = 2 to length(argvals))
basis <- 125 ### Tuneable, if basis=length(arvals) then use raw.
reduced.basis.spline <- create.bspline.basis(range=range(temperatures), nbasis = basis+2)
Smoothing.Matrix <- S.basis(temperatures, basis=reduced.basis.spline)
fdata.smooth <- fdata.raw
fdata.smooth$data <- fdata.raw$data%*%Smoothing.Matrix
plot(fdata.smooth)
cbind(fdata.smooth$data[,1], fdata.raw$data[,1])
fdata.working <- fdata.smooth ### Working functional data set
toolkit.ws <- data.frame(Class = class.raw$SLE, X = fdata.working$data)  ### data.frame ws
colnames(toolkit.ws) <- c('Class', temperatures)
################################
### Data flow/cleaning is all above.
### Below is me starting to work on functions that should call the working data set.
################################
######### Function #############
## I will start by doing something outside the scope of 578.  I will use Penalized-LDA
library('penalizedLDA')
fdata.working <- fdata.raw ### Working functional data set
x <- fdata.working$data
dim(x)
y <- as.numeric(class.raw$SLE)
penlda.cv.out <- PenalizedLDA.cv(x=x, y=y)
lambda.seq <- exp(seq(-6, 2, 0.01))  ### This could be an input, best to suggest evalauating this outide of functions.
penlda.cv.out <- PenalizedLDA.cv(x=x, y=y, lambdas=lambda.seq, nfold = 10)
plot(penlda.cv.out)
penlda.cv.out$bestlambda
### The start of a function
set.seed(10)
folds.all <- caret::createFolds(class.raw$SLE)
lambda.seq <- exp(seq(-6, 2, 0.01))  ### This could be an input, best to suggest evalauating this outide of functions.
### I have figured out how to get posterior estimates of the probability for the binary cases.
fold.id <- 2
folds.all[[fold.id]]
x.test <- x[folds.all[[fold.id]],]
y.test <- y[folds.all[[fold.id]]]
x.train <- x[-folds.all[[fold.id]],]
y.train <- y[-folds.all[[fold.id]]]
penlda.cv.out <- PenalizedLDA.cv(x=x.train, y=y.train, lambdas=lambda.seq, nfold = 10)
lambda.set <- penlda.cv.out$bestlambda
penlda.out <- PenalizedLDA(x=x.train, y=y.train, lambda=lambda.set, K=1)
plot(penlda.out$discrim)
plot(seq(45, 90, 0.1), penlda.out$discrim)
xte <- scale(x.test, center = apply(x.train, 2, mean), scale = penlda.out$wcsd.x) #normalize test data
probs <- xte%*%penlda.out$discrim #calculte posterior
probs
predict.test <- probs>0
factor(predict.test, c(TRUE, FALSE), c(0,1))
y.test-1
table(y.test-1, predict.test)
mean(y.test-1==predict.test)
cbind(probs, penlda.preds$ypred)
penlda.preds <- predict(penlda.out, x.test)
acc <- mean(penlda.preds$ypred==y.test)  ### There does not seem to be a way to get posterior probabilities, so we may not include this
predict.penlda
acc
source('C:/Users/Robert/Dropbox/Northern Arizona University/Github_Repos/Thermogram_Toolkit/Data_Import_Cleaning.R')
### This data is created using Data_Import_Cleaning where we can choose smoothing and derivatives.
plot(fdata.working) ### Working functional data set
######### Function #############
## Classification using penalized LDA
library('penalizedLDA')
##### Data Import #####
### This data is created using Data_Import_Cleaning where we can choose smoothing and derivatives.
plot(fdata.working) ### Working functional data set
### Setup
x <- fdata.working$data
y <- as.numeric(class.raw$SLE)
set.seed(10)
folds.all <- caret::createFolds(class.raw$SLE)  ### prefer as input or we randomly generate the folds
lambda.seq <- exp(seq(-6, 2, 0.01))  ### This could be an input, best to suggest evalauating this outide of functions.
fold.id <- 2
folds.all[[fold.id]]
x.test <- x[folds.all[[fold.id]],]
y.test <- y[folds.all[[fold.id]]]
x.train <- x[-folds.all[[fold.id]],]
y.train <- y[-folds.all[[fold.id]]]
penlda.cv.out <- PenalizedLDA.cv(x=x.train, y=y.train, lambdas=lambda.seq, nfold = 10)
lambda.set <- penlda.cv.out$bestlambda
penlda.out <- PenalizedLDA(x=x.train, y=y.train, lambda=lambda.set, K=1)
plot(penlda.cv.out)
lambda.set <- penlda.cv.out$bestlambda
lambda.set <- penlda.cv.out$bestlambda
penlda.out <- PenalizedLDA(x=x.train, y=y.train, lambda=lambda.set, K=1)
xte <- scale(x.test, center = apply(x.train, 2, mean), scale = penlda.out$wcsd.x) #normalize test data
probs.log.temp <- xte%*%penlda.out$discrim #calculte posterior log-odds
probs.temp <- 1/(1+exp(-probs.log.temp))
probs.temp
probs.temp>0
probs.temp>0.5
library("ggroc")
install.packages('ggroc')
install.packages("ggROC")
library("ggROC")
my.roc <- pROC::roc(y.train, probs.temp)
my.roc <- pROC::roc(y.test, probs.temp)
my.roc <- pROC::roc(y.test, as.vector(probs.temp))
ggroc(my.roc)
my.roc$auc
as.numeric(my.roc$auc)
ggroc(my.roc)
plot(my.roc)
library("pROC")
######### Function #############
## Classification using penalized LDA
library('penalizedLDA')
library("pROC")
##### Data Import #####
### This data is created using Data_Import_Cleaning where we can choose smoothing and derivatives.
plot(fdata.working) ### Working functional data set
##### Setup #####
x <- fdata.working$data
y <- as.numeric(class.raw$SLE)
set.seed(10)
folds.k <- 10
folds.all <- caret::createFolds(class.raw$SLE, folds.k)  ### prefer as input or we randomly generate the folds
lambda.seq <- exp(seq(-6, 2, 0.01))  ### This could be an input, best to suggest evalauating this outide of functions.
penlda.out <- invisible(PenalizedLDA(x=x.train, y=y.train, lambda=lambda.set, K=1))
### Run  penlda CV for lambda
penlda.cv.out <- invisible(PenalizedLDA.cv(x=x.train, y=y.train, lambdas=lambda.seq, nfold = 10))
### Run  penlda CV for lambda
penlda.cv.out <- silent(PenalizedLDA.cv(x=x.train, y=y.train, lambdas=lambda.seq, nfold = 10))
### Run  penlda CV for lambda
penlda.cv.out <- silent(PenalizedLDA.cv(x=x.train, y=y.train, lambdas=lambda.seq, nfold = 10, K=2))
### Run  penlda CV for lambda
penlda.cv.out <- PenalizedLDA.cv(x=x.train, y=y.train, lambdas=lambda.seq, nfold = 10, K=2)
### Run  penlda CV for lambda
penlda.cv.out <- PenalizedLDA.cv(x=x.train, y=y.train, lambdas=lambda.seq, nfold = 10)
