# TN.step<-conf.tab[1]
# TP.step<-conf.tab[4]
# spec.out<-TN.step/N0
# sens.out<-TP.step/N1
#
# return(list(predictions=pred.step, accuracy=acc.out, sensitivity=sens.out, specificity=spec.out))
}
#   if(missing.obs==0) conf.tab<-cbind(c(0,0), conf.tab)
#   if(missing.obs==1) conf.tab<-cbind(conf.tab, c(0,0))
# }
#
# TN.step<-conf.tab[1]
# TP.step<-conf.tab[4]
# spec.out<-TN.step/N0
# sens.out<-TP.step/N1
#
# return(list(predictions=pred.step, accuracy=acc.out, sensitivity=sens.out, specificity=spec.out))
return(pred.step)
lr.fit<-function(train.x, test.x, train.classes)
{
N0<-length(which(test.classes==0))
N1<-length(which(test.classes==1))
glm.step<-glm(train.classes~., family="binomial", data=as.data.frame(t(train.x)))
pred.step<-predict.glm(glm.step, as.data.frame(t(test.x)), type="response")
# pred.class<-rep(0,length(test.classes))
# pred.class[which(pred.step>0.5)]<-1
# pred.class[which(pred.step==0.5)]<-sample(c(0,1))
# conf.tab<-table(test.classes, pred.class)
# acc.out<-mean(test.classes==pred.class)
#
# if(length(as.numeric(colnames(conf.tab)))!=2)
# {
#   missing.obs<-setdiff(c(0,1), as.numeric(colnames(conf.tab)))
#   if(missing.obs==0) conf.tab<-cbind(c(0,0), conf.tab)
#   if(missing.obs==1) conf.tab<-cbind(conf.tab, c(0,0))
# }
#
# TN.step<-conf.tab[1]
# TP.step<-conf.tab[4]
# spec.out<-TN.step/N0
# sens.out<-TP.step/N1
#
# return(list(predictions=pred.step, accuracy=acc.out, sensitivity=sens.out, specificity=spec.out))
return(pred.step)
}
lr.fit(train.x, test.x, train.classes)
lr.fit<-function(train.x, test.x, train.classes)
{
glm.step<-glm(train.classes~., family="binomial", data=as.data.frame(t(train.x)))
pred.step<-predict.glm(glm.step, as.data.frame(t(test.x)), type="response")
# pred.class<-rep(0,length(test.classes))
# pred.class[which(pred.step>0.5)]<-1
# pred.class[which(pred.step==0.5)]<-sample(c(0,1))
# conf.tab<-table(test.classes, pred.class)
# acc.out<-mean(test.classes==pred.class)
#
# if(length(as.numeric(colnames(conf.tab)))!=2)
# {
#   missing.obs<-setdiff(c(0,1), as.numeric(colnames(conf.tab)))
#   if(missing.obs==0) conf.tab<-cbind(c(0,0), conf.tab)
#   if(missing.obs==1) conf.tab<-cbind(conf.tab, c(0,0))
# }
#
# TN.step<-conf.tab[1]
# TP.step<-conf.tab[4]
# spec.out<-TN.step/N0
# sens.out<-TP.step/N1
#
# return(list(predictions=pred.step, accuracy=acc.out, sensitivity=sens.out, specificity=spec.out))
return(pred.step)
}
lr.fit(train.x, test.x, train.classes)
train.classes <- factor(class.raw[-folds.temp[[1]]])
train.classes <- factor(class.raw$V452[-folds.temp[[1]]])
lr.fit<-function(train.x, test.x, train.classes)
{
glm.step<-glm(train.classes~., family="binomial", data=as.data.frame(t(train.x)))
pred.step<-predict.glm(glm.step, as.data.frame(t(test.x)), type="response")
# pred.class<-rep(0,length(test.classes))
# pred.class[which(pred.step>0.5)]<-1
# pred.class[which(pred.step==0.5)]<-sample(c(0,1))
# conf.tab<-table(test.classes, pred.class)
# acc.out<-mean(test.classes==pred.class)
#
# if(length(as.numeric(colnames(conf.tab)))!=2)
# {
#   missing.obs<-setdiff(c(0,1), as.numeric(colnames(conf.tab)))
#   if(missing.obs==0) conf.tab<-cbind(c(0,0), conf.tab)
#   if(missing.obs==1) conf.tab<-cbind(conf.tab, c(0,0))
# }
#
# TN.step<-conf.tab[1]
# TP.step<-conf.tab[4]
# spec.out<-TN.step/N0
# sens.out<-TP.step/N1
#
# return(list(predictions=pred.step, accuracy=acc.out, sensitivity=sens.out, specificity=spec.out))
return(pred.step)
}
lr.fit(train.x, test.x, train.classes)
lr.fit<-function(train.x, test.x, train.classes)
{
glm.step<-glm(train.classes~., family="binomial", data=as.data.frame(t(train.x)))
pred.step<-predict.glm(glm.step, as.data.frame(t(test.x)), type="response")
# pred.class<-rep(0,length(test.classes))
# pred.class[which(pred.step>0.5)]<-1
# pred.class[which(pred.step==0.5)]<-sample(c(0,1))
# conf.tab<-table(test.classes, pred.class)
# acc.out<-mean(test.classes==pred.class)
#
# if(length(as.numeric(colnames(conf.tab)))!=2)
# {
#   missing.obs<-setdiff(c(0,1), as.numeric(colnames(conf.tab)))
#   if(missing.obs==0) conf.tab<-cbind(c(0,0), conf.tab)
#   if(missing.obs==1) conf.tab<-cbind(conf.tab, c(0,0))
# }
#
# TN.step<-conf.tab[1]
# TP.step<-conf.tab[4]
# spec.out<-TN.step/N0
# sens.out<-TP.step/N1
#
# return(list(predictions=pred.step, accuracy=acc.out, sensitivity=sens.out, specificity=spec.out))
return(pred.step)
}
lr.fit(train.x, test.x, train.classes)
lr.fit<-function(train.x, test.x, train.classes)
{
glm.step<-glm(train.classes~., family="binomial", data=as.data.frame(train.x))
pred.step<-predict.glm(glm.step, as.data.frame(t(test.x)), type="response")
# pred.class<-rep(0,length(test.classes))
# pred.class[which(pred.step>0.5)]<-1
# pred.class[which(pred.step==0.5)]<-sample(c(0,1))
# conf.tab<-table(test.classes, pred.class)
# acc.out<-mean(test.classes==pred.class)
#
# if(length(as.numeric(colnames(conf.tab)))!=2)
# {
#   missing.obs<-setdiff(c(0,1), as.numeric(colnames(conf.tab)))
#   if(missing.obs==0) conf.tab<-cbind(c(0,0), conf.tab)
#   if(missing.obs==1) conf.tab<-cbind(conf.tab, c(0,0))
# }
#
# TN.step<-conf.tab[1]
# TP.step<-conf.tab[4]
# spec.out<-TN.step/N0
# sens.out<-TP.step/N1
#
# return(list(predictions=pred.step, accuracy=acc.out, sensitivity=sens.out, specificity=spec.out))
return(pred.step)
}
lr.fit(train.x, test.x, train.classes)
lr.fit<-function(train.x, test.x, train.classes)
{
glm.step<-glm(train.classes~., family="binomial", data=as.data.frame(train.x))
pred.step<-predict.glm(glm.step, as.data.frame(test.x), type="response")
# pred.class<-rep(0,length(test.classes))
# pred.class[which(pred.step>0.5)]<-1
# pred.class[which(pred.step==0.5)]<-sample(c(0,1))
# conf.tab<-table(test.classes, pred.class)
# acc.out<-mean(test.classes==pred.class)
#
# if(length(as.numeric(colnames(conf.tab)))!=2)
# {
#   missing.obs<-setdiff(c(0,1), as.numeric(colnames(conf.tab)))
#   if(missing.obs==0) conf.tab<-cbind(c(0,0), conf.tab)
#   if(missing.obs==1) conf.tab<-cbind(conf.tab, c(0,0))
# }
#
# TN.step<-conf.tab[1]
# TP.step<-conf.tab[4]
# spec.out<-TN.step/N0
# sens.out<-TP.step/N1
#
# return(list(predictions=pred.step, accuracy=acc.out, sensitivity=sens.out, specificity=spec.out))
return(pred.step)
}
lr.fit(train.x, test.x, train.classes)
all.x <- fdata.working$data
folds.temp <- caret::createFolds(class.raw)
train.x <- all.x[-folds.temp[[1]],]
test.x <- all.x[folds.temp[[1]],]
train.classes <- factor(class.raw$V452[-folds.temp[[1]]])
glm.step<-glm(train.classes~., family="binomial", data=as.data.frame(train.x))4
glm.step<-glm(train.classes~., family="binomial", data=as.data.frame(train.x))
pred.step<-predict.glm(glm.step, as.data.frame(test.x), type="response")
-folds.temp[[1]]
folds.temp
folds.temp <- caret::createFolds(class.raw$V452)
train.x <- all.x[-folds.temp[[1]],]
test.x <- all.x[folds.temp[[1]],]
train.classes <- factor(class.raw$V452[-folds.temp[[1]]])
lr.fit<-function(train.x, test.x, train.classes)
{
glm.step<-glm(train.classes~., family="binomial", data=as.data.frame(train.x))
pred.step<-predict.glm(glm.step, as.data.frame(test.x), type="response")
# pred.class<-rep(0,length(test.classes))
# pred.class[which(pred.step>0.5)]<-1
# pred.class[which(pred.step==0.5)]<-sample(c(0,1))
# conf.tab<-table(test.classes, pred.class)
# acc.out<-mean(test.classes==pred.class)
#
# if(length(as.numeric(colnames(conf.tab)))!=2)
# {
#   missing.obs<-setdiff(c(0,1), as.numeric(colnames(conf.tab)))
#   if(missing.obs==0) conf.tab<-cbind(c(0,0), conf.tab)
#   if(missing.obs==1) conf.tab<-cbind(conf.tab, c(0,0))
# }
#
# TN.step<-conf.tab[1]
# TP.step<-conf.tab[4]
# spec.out<-TN.step/N0
# sens.out<-TP.step/N1
#
# return(list(predictions=pred.step, accuracy=acc.out, sensitivity=sens.out, specificity=spec.out))
return(pred.step)
}
lr.fit(train.x, test.x, train.classes)
### Libraries
library(caret)
library(fda)
library(fda.usc)
library(dplyr)
### Data
### Establish a clear set of inputs that can be used generally for statistical estimation.
### We can also start working to establish .Rdata files that have the data frame stored correctly.
data.raw <- read.table('lupustherm.txt')
temperatures <- seq(45, 90, 0.1)
thermogram.length <- length(temperatures)
thermograms.raw <- data.raw %>% select(1:thermogram.length)
class.raw <- data.raw %>% select(-(1:thermogram.length))
thermogram.fda <- function(x, argvals, nbasis)
{
if(typeof(x)=='list') x<-as.matrix(x)
thermogram.length <- length(argvals)
samples.n <- dim(x)[1]
fdata.raw <- fdata(x, argvals=argvals)
reduced.basis.spline <- create.bspline.basis(range=range(argvals), nbasis = nbasis)
tryCatch(Smoothing.Matrix <- S.basis(argvals, basis=reduced.basis.spline),
error=function(x) {Smoothing.Matrix<<-diag(thermogram.length)})
### Tryign to decide how to handle if the 'smoothing' is singular
### Need to just return an unsmoothed matrix, but its global and I can't find another way yet.
fdata.smooth <- fdata.raw
fdata.smooth$data <- fdata.raw$data%*%Smoothing.Matrix
return(fdata.smooth)
}
fdata.working <- thermogram.fda(thermograms.raw, temperatures, 450)
fdata.working <- thermogram.fda(thermograms.raw, temperatures, 300)
class.raw <- factor(data.raw %>% select(thermogram.length+1))
data.raw <- read.table('lupustherm.txt')
temperatures <- seq(45, 90, 0.1)
thermogram.length <- length(temperatures)
thermograms.raw <- data.raw %>% select(1:thermogram.length)
class.raw <- factor(data.raw %>% select(thermogram.length+1))
data.raw %>% select(thermogram.length+1)
class.raw <- data.raw %>% select(thermogram.length+1) %>% factor()
class.raw <- data.raw %>% select(thermogram.length+1)
class.raw <- as.vector(data.raw %>% select(thermogram.length+1))
class.raw <- as.vector(data.raw %>% select(thermogram.length+1))
class.raw <- as.numeric(data.raw %>% select(thermogram.length+1))
lr.fit<-function(train.x, test.x, train.classes)
{
glm.step<-glm(train.classes~., family="binomial", data=as.data.frame(train.x))
return(glm.step)
}
lr.fit(train.x, test.x, train.classes)
all.x <- fdata.working$data
folds.temp <- caret::createFolds(class.raw$V452)
train.x <- all.x[-folds.temp[[1]],]
test.x <- all.x[folds.temp[[1]],]
train.classes <- factor(class.raw$V452[-folds.temp[[1]]])
lr.fit<-function(train.x, test.x, train.classes)
{
glm.step<-glm(train.classes~., family="binomial", data=as.data.frame(train.x))
return(glm.step)
}
lr.fit(train.x, test.x, train.classes)
classes <- class.raw$V452
folds.list <- replicate(10,caret::createFolds(class.raw$V452))
folds.list <- replicate(10,caret::createFolds(classes))
### Libraries
library(caret)
library(fda)
library(fda.usc)
library(dplyr)
### Data
### Establish a clear set of inputs that can be used generally for statistical estimation.
### We can also start working to establish .Rdata files that have the data frame stored correctly.
data.raw <- read.table('lupustherm.txt')
temperatures <- seq(45, 90, 0.1)
thermogram.length <- length(temperatures)
thermograms.raw <- data.raw %>% select(1:thermogram.length)
class.raw <- data.raw %>% select(thermogram.length+1)
### thermogram.fda
### A function that takes thermogram data and produces a 'smoothed' version based on nbasis or lambda-pen
###
### x : a matrix (for thermograms this should be ...), accepta a data.frame also
### argvals : the temperatures
### basis.size : the n.basis used to produce fda representation
###
### NEEDS: Derivative.
thermogram.fda <- function(x, argvals, nbasis)
{
if(typeof(x)=='list') x<-as.matrix(x)
thermogram.length <- length(argvals)
samples.n <- dim(x)[1]
fdata.raw <- fdata(x, argvals=argvals)
reduced.basis.spline <- create.bspline.basis(range=range(argvals), nbasis = nbasis)
tryCatch(Smoothing.Matrix <- S.basis(argvals, basis=reduced.basis.spline),
error=function(x) {Smoothing.Matrix<<-diag(thermogram.length)})
### Tryign to decide how to handle if the 'smoothing' is singular
### Need to just return an unsmoothed matrix, but its global and I can't find another way yet.
fdata.smooth <- fdata.raw
fdata.smooth$data <- fdata.raw$data%*%Smoothing.Matrix
return(fdata.smooth)
}
fdata.working <- thermogram.fda(thermograms.raw, temperatures, 300)
predictor.set <- fdata.working$data
classes <- class.raw$V452
folds.list <- replicate(10,caret::createFolds(classes))
folds.list[[1]]
### loop over folds
train.x <- all.x[-folds.temp[[1]],]
folds.list[[100]]
### loop over iterations
i=1
### loop over folds
fld=1
folds.list <- caret::createFolds(classes)
fold.creator<-function(classes, folds, trials)
{
folds.list<-list()
for(j in  1:trials)
folds.list[[j]]<-createFolds(factor(classes), k=folds, list=FALSE)
return(folds.list)
}
fold.creator<-function(classes, folds, trials)
{
folds.list<-list()
for(j in  1:trials)
folds.list[[j]]<-caret::createFolds(factor(classes), k=folds, list=FALSE)
return(folds.list)
}
predictor.set <- fdata.working$data
classes <- class.raw$V452
folds <- 10
repeats <- 10
folds.list <- function(classes, folds, repeats)
fold.creator<-function(classes, folds=10, trials=1)
{
folds.list<-list()
for(j in  1:trials)
folds.list[[j]]<-caret::createFolds(factor(classes), k=folds, list=FALSE)
return(folds.list)
}
predictor.set <- fdata.working$data
classes <- class.raw$V452
folds <- 10
repeats <- 10
folds.list <- function(classes, folds, repeats)
folds.list[[100]]
folds.list <- function(classes, folds, repeats)
folds.list <- fold.creator(classes, folds, repeats)
folds.list[[100]]
folds.list[[1]][[10]]
fold.creator<-function(classes, folds=10, trials=1)
{
folds.list<-list()
for(j in  1:trials)
folds.list[[j]]<-caret::createFolds(factor(classes), k=folds, list=FALSE)
return(folds.list)
}
### Logistic Regression
### Function for performing LR and cross-validation.
### Returns the resulting predictions of test.x
predictor.set <- fdata.working$data
classes <- class.raw$V452
folds <- 10
repeats <- 10
folds.list <- fold.creator(classes, folds, repeats)
folds.list[[1]][[10]]
### Libraries
library(caret)
library(fda)
library(fda.usc)
library(dplyr)
### Data
### Establish a clear set of inputs that can be used generally for statistical estimation.
### We can also start working to establish .Rdata files that have the data frame stored correctly.
data.raw <- read.table('lupustherm.txt')
temperatures <- seq(45, 90, 0.1)
thermogram.length <- length(temperatures)
thermograms.raw <- data.raw %>% select(1:thermogram.length)
class.raw <- data.raw %>% select(thermogram.length+1)
### thermogram.fda
### A function that takes thermogram data and produces a 'smoothed' version based on nbasis or lambda-pen
###
### x : a matrix (for thermograms this should be ...), accepta a data.frame also
### argvals : the temperatures
### basis.size : the n.basis used to produce fda representation
###
### NEEDS: Derivative.
thermogram.fda <- function(x, argvals, nbasis)
{
if(typeof(x)=='list') x<-as.matrix(x)
thermogram.length <- length(argvals)
samples.n <- dim(x)[1]
fdata.raw <- fdata(x, argvals=argvals)
reduced.basis.spline <- create.bspline.basis(range=range(argvals), nbasis = nbasis)
tryCatch(Smoothing.Matrix <- S.basis(argvals, basis=reduced.basis.spline),
error=function(x) {Smoothing.Matrix<<-diag(thermogram.length)})
### Tryign to decide how to handle if the 'smoothing' is singular
### Need to just return an unsmoothed matrix, but its global and I can't find another way yet.
fdata.smooth <- fdata.raw
fdata.smooth$data <- fdata.raw$data%*%Smoothing.Matrix
return(fdata.smooth)
}
fdata.working <- thermogram.fda(thermograms.raw, temperatures, 300)
# rm(Smoothing.Matrix)
### Fold Creator
fold.creator<-function(classes, folds=10, trials=1)
{
folds.list<-list()
for(j in  1:trials)
folds.list[[j]]<-caret::createFolds(factor(classes), k=folds, list=FALSE)
return(folds.list)
}
predictor.set <- fdata.working$data
classes <- class.raw$V452
folds <- 10
repeats <- 10
folds.list <- fold.creator(classes, folds, repeats)
fold.creator(classes, folds, repeats)
fold.creator(classes, folds, trials=repeats)
fold.creator<-function(classes, folds=10, trials=10)
{
folds.list<-list()
for(j in  1:trials)
folds.list[[j]]<-caret::createFolds(factor(classes), k=folds, list=FALSE)
return(folds.list)
}
predictor.set <- fdata.working$data
classes <- class.raw$V452
folds <- 10
repeats <- 10
fold.creator(classes, folds, trials=repeats)
fold.creator(classes, folds, trials=10)
fold.creator(classes, folds, trials=3)
fold.creator(classes, folds=10, trials=3)
fold.creator<-function(classes, folds=10, trials=10)
{
folds.list<-list()
for(j in  1:trials)
folds.list[[j]]<-caret::createFolds(factor(classes), k=folds, list=FALSE)
return(folds.list)
}
fold.creator(classes, folds, repeats)
folds <- 3
repeats <- 10
fold.creator(classes, folds, repeats)
folds <- 10
repeats <- 10
fold.creator(classes, folds, repeats)
folds.list <- fold.creator(classes, folds, repeats)
training.index <- which(folds.list[[i]]!=fld)
### loop over iterations
i=1
### loop over folds
fld=1
training.index <- which(folds.list[[i]]!=fld)
train.classes <- classes[train.index]
### loop over iterations
i=1
### loop over folds
fld=1
train.index <- which(folds.list[[i]]!=fld)
train.x <- predictor.set[train.index,]
test.x <- predictor.set[-train.index,]
train.classes <- classes[train.index]
lr.fit<-function(train.x, test.x, train.classes)
{
glm.step<-glm(train.classes~., family="binomial", data=as.data.frame(train.x))
return(glm.step)
}
lr.fit(train.x, test.x, train.classes)
model.chosen='lr.fit'
model.chosen(train.x, test.x, train.classes)
func<-lr.fit
func(train.x, test.x, train.classes)
if(model.chosen=='lr.fit') func<-lr.fit
model.result<-func(train.x, test.x, train.classes)
coefficients <- model.result$coefficients
plot(coefficients)
coefficients <- model.result$coefficients[-1]
plot(coefficients)
predicions <- predict.glm(model.result, as.data.frame(test.x), type="response")
plot(predictions)
predicions <- predict.glm(model.result, as.data.frame(test.x), type="response")
model.result<-func(train.x, test.x, train.classes)
if(model.chosen=='lr.fit')
{
coefficients <- model.result$coefficients[-1]
predicions <- predict.glm(model.result, as.data.frame(test.x), type="response")
}
plot(predictions)
model.result<-func(train.x, test.x, train.classes)
if(model.chosen=='lr.fit')
{
coefficients <- model.result$coefficients[-1]
predictions <- predict.glm(model.result, as.data.frame(test.x), type="response")
}
plot(predictions)
